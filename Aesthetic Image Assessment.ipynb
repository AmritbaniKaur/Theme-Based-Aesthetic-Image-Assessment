{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theme-based Aesthetic Image Assessment\n",
    "\n",
    "With the surge of digital device and smart phones, billions of photos are collected everyday. For people who cares about the beauty or the feeling of photos, they hope to be instructed when they are taking or modifying photos to enhance aesthetic values. Also, people love to share photos and watch others' photos in social networks they hope to share and watch the high-quality photos and got mediocre photos filtered out. Therefore, a trained 'aesthetic evaluator' will give them a pre-judgement about the 'beauty' of the photos, help them create, select best photos.\n",
    "\n",
    "## Project Content\n",
    "In this project, We will collect image dataset first, and then implement 2 feature extraction approaches, convolutional neural network(CNN) with triple loss function and VGG pretrained network. All these featurets would be test on SVM classfier and compared with each other in the end.\n",
    "\n",
    "1. Data Preparation \n",
    "2. CNN with Triplet loss function\n",
    "4. Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mimetypes, httplib, time, sys, os\n",
    "import unittest\n",
    "import urllib2\n",
    "import cv2\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "According to the criteria, we chose the photos in Photo.net to train and test the algorithm. Photo.net is a great online photo sharing community with over 400,000 active photographers and they’re constantly doing peer-rating for each other’s works.\n",
    "\n",
    "To fetch the image data from photo.net, we have parsed the html page of photo.net, collected photos ids and scores information and downloaded image by id. In addition, the size of images in photo.net are different, we have croped and scaled images to have same size.\n",
    "\n",
    "we choosed to have images in travel and lanscape in our category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage(url):\n",
    "    \n",
    "    #Browser Information\n",
    "    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "\n",
    "    request = urllib2.Request(url,headers = headers)\n",
    "\n",
    "    #Read response and open the url\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    #Get web page HTML code and decode it\n",
    "    pageCode = response.read().decode('utf-8')\n",
    "\n",
    "    return pageCode\n",
    "def parse_page(html,ids,rates):\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    photos = soup.find_all(\"div\", attrs = {'class':'trp_photo'})\n",
    "    for photo in photos:\n",
    "        id = photo.find('a')['href'].split('=')[1]\n",
    "        rate = str(photo.find('div',attrs={'class':'trp-details'}))\n",
    "        l = rate.find(\"<strong>Rating:</strong>\")\n",
    "        r = rate.find(\"<br>\",l)\n",
    "        rate = rate[l:r].split(' ')[-1]\n",
    "        if(len(rate)<2):\n",
    "            continue\n",
    "        if id in ids:\n",
    "            continue\n",
    "        ids.append(id)\n",
    "        rates.append(float(rate))\n",
    "    return (ids,rates)\n",
    "    pass\n",
    "\n",
    "ids = []\n",
    "rates = []\n",
    "for page_index in range(0,3000,12):\n",
    "    for category in ['Travel','Landscape']:\n",
    "        for period in ['90','365','365-1','365-2','365-3','365-4','365-5','5000']:\n",
    "            url = \"http://photo.net/gallery/photocritique/filter?period=\"+ period \n",
    "            url += \"&rank_by=avg&category=\" + category + \"&start_index=\" + str(page_index) \n",
    "            url += \"&store_prefs_p=1&shown_tab=1&page=Next\"\n",
    "            html = getPage(url)\n",
    "            [ids,rates] = parse_page(html,ids,rates)\n",
    "print(\"number of different image ids obtained: \",len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After crawling the images, we visualize the score distribution to have a brief understanding of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score distribution visualization\n",
    "scores = rates\n",
    "rate = {}\n",
    "for score in scores:\n",
    "    score = int(score*10)\n",
    "    if score not in rate:\n",
    "        rate[score] = 0\n",
    "    rate[score] += 1\n",
    "sd = sorted(rate.items())\n",
    "X = []\n",
    "Y = []\n",
    "for a,b in sd:\n",
    "    X.append(a/10.0)\n",
    "    Y.append(b)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Score distribution of imgs by photo.net')\n",
    "plt.plot(X,Y)\n",
    "plt.savefig('ds_all.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download image data\n",
    "To better prepare the datasets used for training and classification, we only collected images from two tails. Specifically, we have 12% negative examples from least rated side and another 12% examples from high rated side. Therefore, we define images with rating score below 4.0 as “negative” image, and images with scores over 6.0 as ”positive” images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download photos\n",
    "for i in range(len(ids)):\n",
    "    id = ids[i]\n",
    "    score = rates[i]\n",
    "    if (score<6.0 and score>4.0):\n",
    "        continue\n",
    "    url = 'http://gallery.photo.net/photo/'+id+'-md.jpg'\n",
    "    urllib.urlretrieve(url,\"dataset/\"+id+\"_\"+str(score)+\".jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing and Extracting names from file names\n",
    "We observed different size in different images. Therefore we have cropped images into a square as 96 x 96 pixels image. Also, we parsed the labels from filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_scale_image(im):\n",
    "    if im.mode is not 'RGB':\n",
    "        im = im.convert('RGB')\n",
    "    width,height = im.size\n",
    "    if width > height:\n",
    "        diff = width - height\n",
    "        box = diff/2, 0, width - (diff - diff/2), height\n",
    "    else:\n",
    "        diff = height - width\n",
    "        box = 0, diff/2, width, height - (diff - diff/2)\n",
    "    im = im.crop(box)\n",
    "    toSize = 96,96\n",
    "    im= im.resize(toSize, Image.ANTIALIAS)\n",
    "    return im\n",
    "\n",
    "def fnames_to_labels(fnames):\n",
    "    res = []\n",
    "    for fname in fnames:\n",
    "        score = float(fname.split('_')[1].split('.jpeg')[0])\n",
    "        if score > 5:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(-1)\n",
    "    return np.asarray(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into Train / Validate / Test splits\n",
    "Next we have loaded all the 9047 images (4503 pos and 4544 neg) and perform our usual data split. We splited these photos into three categories - 1000 for validation, 500 for testing, 7547 for training.\n",
    "\n",
    "dataset is a folder that has all the images downloaded from the photo.net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "dname = \"dataset/\"\n",
    "im_paths = [dname+fname for fname in os.listdir(dname) if fname.endswith(\".jpeg\")]\n",
    "im_paths = np.array(im_paths)\n",
    "random.shuffle(im_paths)\n",
    "im_labels = fnames_to_labels(im_paths)\n",
    "\n",
    "im_paths_pos = im_paths[im_labels>0]\n",
    "im_paths_neg = im_paths[im_labels<0]\n",
    "print(len(im_paths_pos),len(im_paths_neg))\n",
    "fnames_te = np.concatenate((im_paths_pos[0:250], im_paths_neg[:250]))\n",
    "fnames_va = np.concatenate((im_paths_pos[250:750], im_paths_neg[250:750]))\n",
    "fnames_tr = np.concatenate((im_paths_pos[750:len(im_paths_neg)], im_paths_neg[750:]))\n",
    "print (\"Train data size: \",len(fnames_tr))\n",
    "print (\"Validation data size: \",len(fnames_va))\n",
    "print (\"Test data size: \",len(fnames_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with  loss function\n",
    "Inspired from Understanding Aesthetics with Deep Learning, a journal form NVIDIA, the challenging problem can be approached by training the Convolution Neural Network of the defined the triplet loss:\n",
    "\n",
    "Triple loss =  $max(0, c + Dist( \\phi(I_1), \\phi(I_2)) - Dist(\\phi(I_1), \\phi(I_3) )$\n",
    " \n",
    "The objective of this loss function is let CNN learn the similarity between high-quality images, and learn the difference between high-quality and mediocre images. Therefore, by training the network, we hope to learn a feature representation  ϕ(.)ϕ(.)  that the feature distance between two high-quality images is smaller than the feature distance between one high-quality image and one low-quality image. Also, by introducing the margin  cc , we can train the network such that the distance between  ϕ(I1)ϕ(I1)  and  ϕ(I3)ϕ(I3)  is greater by c than the distance between  ϕ(I1)ϕ(I1)  and  ϕ(I2)ϕ(I2) .\n",
    "\n",
    "It is noticeable we are not going to learn the classifier by the triplet loss function and deep network, we are learning features representation method  ϕ(.)ϕ(.) . Once we learned  ϕ(.)ϕ(.) , we will use SVM to train the classifier using the feature representation of images.\n",
    "\n",
    "\n",
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np      \n",
    "from PIL import Image   \n",
    "import random           \n",
    "import time             \n",
    "from sklearn import svm \n",
    "import logging          \n",
    "\n",
    "dataset = './dataset4'\n",
    "logging.basicConfig(filename='cnnlog017.log',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train set                                             \n",
    "posImg_ids_Tr = []                                              \n",
    "posImg_scores_Tr = []                                           \n",
    "negImg_ids_Tr = []                                              \n",
    "negImg_scores_Tr = []                                           \n",
    "with open( dataset + '/ratingsInfo/train.csv', 'r') as f:       \n",
    "    f.readline()                                                \n",
    "    for line in f.readlines():                                  \n",
    "        img_id,img_score,img_label = line.strip().split(',')    \n",
    "        if float(img_label) > 0:                                \n",
    "            posImg_ids_Tr.append(img_id)                        \n",
    "            posImg_scores_Tr.append(img_score)                  \n",
    "        else:                                                   \n",
    "            negImg_ids_Tr.append(img_id)                        \n",
    "            negImg_scores_Tr.append(img_score)                  \n",
    "print (\"Train Set Positive Images:\",len(posImg_ids_Tr))\n",
    "print (\"Train Set Negative Images:\",len(negImg_ids_Tr))                                                   \n",
    "# Reading validation set                                        \n",
    "posImg_ids_Va = []                                              \n",
    "posImg_scores_Va = []                                           \n",
    "negImg_ids_Va = []                                              \n",
    "negImg_scores_Va = []                                           \n",
    "with open(dataset + '/ratingsInfo/validation.csv', 'r') as f:   \n",
    "    f.readline()                                                \n",
    "    for line in f.readlines():                                  \n",
    "        img_id,img_score,img_label = line.strip().split(',')    \n",
    "        if float(img_label) > 0:                                \n",
    "            posImg_ids_Va.append(img_id)                        \n",
    "            posImg_scores_Va.append(img_score)                  \n",
    "        else:                                                   \n",
    "            negImg_ids_Va.append(img_id)                        \n",
    "            negImg_scores_Va.append(img_score)                  \n",
    "            \n",
    "print (\"Validation Set Positive Images: \",len(posImg_ids_Va))\n",
    "print (\"Validation Set Negative Images: \", len(negImg_ids_Va))\n",
    "\n",
    "# Reading the test set                                          \n",
    "posImg_ids_Te = []                                              \n",
    "negImg_ids_Te = []                                              \n",
    "posImg_scores_Te = []                                           \n",
    "negImg_scores_Te = []                                           \n",
    "with open(dataset + '/ratingsInfo/test.csv', 'r') as f:         \n",
    "    f.readline()                                                \n",
    "    for line in f.readlines():                                  \n",
    "        img_id,img_score,img_label = line.strip().split(',')    \n",
    "        if float(img_label) > 0:                                \n",
    "            posImg_ids_Te.append(img_id)                        \n",
    "            posImg_scores_Te.append(img_score)                  \n",
    "        else:                                                   \n",
    "            negImg_ids_Te.append(img_id)                        \n",
    "            negImg_scores_Te.append(img_score)    \n",
    "\n",
    "print (\"Test Set Positive Images: \", len(posImg_ids_Te))\n",
    "print (\"Test Set Negative Images: \", len(negImg_ids_Te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImgs_intoNpArray(path, image_id_list, image_score_list):\n",
    "    filenames = []\n",
    "    for img_id, img_score in zip(image_id_list, image_score_list):\n",
    "        name = path + img_id + '_' + img_score + '.jpeg'\n",
    "        filenames.append(name)\n",
    "    imgs = np.array([np.array(Image.open(fname)) for fname in filenames])\n",
    "    return imgs\n",
    "\n",
    "\n",
    "path_Tr = dataset + '/train/'                                              \n",
    "pos_img_Tr = readImgs_intoNpArray(path_Tr, posImg_ids_Tr, posImg_scores_Tr)\n",
    "neg_img_Tr = readImgs_intoNpArray(path_Tr, negImg_ids_Tr, negImg_scores_Tr)\n",
    "anchor_img_Tr = readImgs_intoNpArray(path_Tr, posImg_ids_Tr, posImg_scores_Tr)\n",
    "pos_list_Tr = range(len(pos_img_Tr))                                       \n",
    "neg_list_Tr = range(len(neg_img_Tr))                                       \n",
    "                                                                           \n",
    "path_Va = dataset + '/validation/'                                         \n",
    "pos_img_Va = readImgs_intoNpArray(path_Va, posImg_ids_Va, posImg_scores_Va)\n",
    "neg_img_Va = readImgs_intoNpArray(path_Va, negImg_ids_Va, negImg_scores_Va)\n",
    "                                                                           \n",
    "path_Te = dataset + '/test/'                                               \n",
    "pos_img_Te = readImgs_intoNpArray(path_Te, posImg_ids_Te, posImg_scores_Te)\n",
    "neg_img_Te = readImgs_intoNpArray(path_Te, negImg_ids_Te, negImg_scores_Te)\n",
    "\n",
    "print (\"Positive Training Images size \", pos_img_Tr.shape)\n",
    "print (\"Negative Training Images size \", neg_img_Tr.shape)\n",
    "\n",
    "print (\"Positive Validation Images size \", pos_img_Va.shape)\n",
    "print (\"Negative Training Images size \", neg_img_Va.shape)\n",
    "\n",
    "print (\"Positive Test Images size \", pos_img_Te.shape)\n",
    "print (\"Negative Test Images size \", neg_img_Te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):                                       \n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)                \n",
    "  return tf.Variable(initial)                                     \n",
    "                                                                  \n",
    "def bias_variable(shape):                                         \n",
    "  initial = tf.constant(0.1, shape=shape)                         \n",
    "  return tf.Variable(initial)                                     \n",
    "                                                                  \n",
    "def max_pool_2x2(x):                                              \n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],                    \n",
    "                        strides=[1, 2, 2, 1], padding='SAME')     \n",
    "def avg_pool_2x2(x):                                              \n",
    "  return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1],                    \n",
    "                        strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "def predict_from_features(X_train, y, X_test, SVMReg = 1):                                              \n",
    "                                                                                                        \n",
    "    # C = 1 # SVM regularization parameter                                                              \n",
    "                                                                                                        \n",
    "    lin_svc = svm.SVC(kernel='linear', C=SVMReg, verbose=True, max_iter= 1000).fit(X_train, y)       \n",
    "                                                                                                        \n",
    "    Te_res = lin_svc.predict(X_test)                                                                    \n",
    "    Tr_res = lin_svc.predict(X_train)                                                                   \n",
    "    return Te_res, Tr_res                                                                               \n",
    "                                                                                                        \n",
    "# featuresRes1: features positive                                                                       \n",
    "# featuresRes2: features negative                                                                       \n",
    "def doSVM(featuresRes_pos_Tr,featuresRes_neg_Tr, featuresRes_pos_Te, featuresRes_neg_Te, SVMReg = 1):   \n",
    "    TrainExamplesNum = len(featuresRes_pos_Tr) / 2;                                                     \n",
    "    posTrainX =featuresRes_pos_Tr[:TrainExamplesNum,:]                                                  \n",
    "    posTrainy = np.ones( len(posTrainX) )                                                               \n",
    "                                                                                                        \n",
    "    posTestX =featuresRes_pos_Te                                                                        \n",
    "    posTesty = np.ones( len(posTestX) )                                                                 \n",
    "                                                                                                                                                                       \n",
    "    negTrainX = featuresRes_neg_Tr                                                                      \n",
    "    negTrainy = -1 * np.ones( len(negTrainX) )                                                          \n",
    "    negTestX = featuresRes_neg_Te                                                                       \n",
    "    negTesty = -1 * np.ones( len(negTestX) )                                                            \n",
    "                                                                                                        \n",
    "    trainX =  np.concatenate((posTrainX, negTrainX), axis=0)                                            \n",
    "    trainX = trainX.astype(float)                                                                       \n",
    "    trainy =  np.concatenate((posTrainy, negTrainy))                                                    \n",
    "                                                                                                        \n",
    "    testX =  np.concatenate((posTestX, negTestX), axis=0)                                               \n",
    "    testX = testX.astype(float)                                                                         \n",
    "    testy =  np.concatenate((posTesty, negTesty))                                                       \n",
    "                                                                                                        \n",
    "    start = time.time()                                                                                 \n",
    "    y_p, y_p_tr = predict_from_features(trainX, trainy, testX, SVMReg)                                  \n",
    "    # np.savetxt(\"predict.csv\", y_p, delimiter=',')                                                     \n",
    "    # y_p_tr = predict_from_features(trainX, trainy, testX, max_iter)                                   \n",
    "    end = time.time()                                                                                   \n",
    "    recallT = 0.0                                                                                       \n",
    "    recallF = 0.0                                                                                       \n",
    "    try:                                                                                                \n",
    "        for i in range(len(testy)):                                                                     \n",
    "            if (y_p[i] == testy[i] and y_p[i]>0):                                                       \n",
    "                recallT += 1                                                                            \n",
    "            if (y_p[i] == testy[i] and y_p[i]<0):                                                       \n",
    "                recallF += 1                                                                            \n",
    "        recall = [recallT/(testy>0).sum(),recallF/(testy<0).sum()]                                      \n",
    "    except:                                                                                             \n",
    "        recall = []                                                                                     \n",
    "                                                                                                        \n",
    "    train_acc = np.mean(y_p_tr == trainy)                                                                                                                                                                    \n",
    "    return np.mean(y_p == testy), end-start, recall, y_p, train_acc                                     \n",
    "\n",
    "def RunAndTest( pos_img, neg_img, anchor_img, pos_list, neg_list, pos_img_Te, ne_img_Te, pos_img_Va, neg_img_Va, margin=1000, keepProb = 0.2, learningRate=1e-4, iter_time=200, batch_size = 16, SVMReg = 1):\n",
    "                                                                                                                                                                                                             \n",
    "    sess = tf.InteractiveSession()                                                                                                                                                                           \n",
    "    width = 96                                                                                                                                                                                               \n",
    "    height = 96                                                                                                                                                                                              \n",
    "    channel = 3                                                                                                                                                                                              \n",
    "                                                                                                                                                                                                             \n",
    "    anchor_input = tf.placeholder(tf.float32, [None, width, height, channel])                                                                                                                                \n",
    "    positive_input = tf.placeholder(tf.float32, [None, width, height, channel])                                                                                                                              \n",
    "    negative_input = tf.placeholder(tf.float32, [None, width, height, channel])                                                                                                                              \n",
    "    global_step = tf.Variable(0, trainable=False)                                                                                                                                                            \n",
    "                                                                                                                                                                                                             \n",
    "                                                                                                                                                                                                             \n",
    "    # Initialize variables                                                                                                                                                                                   \n",
    "    W_conv1 = weight_variable([5, 5, 3, 32])                                                                                                                                                                 \n",
    "    b_conv1 = bias_variable([32])                                                                                                                                                                            \n",
    "                                                                                                                                                                                                             \n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])                                                                                                                                                                \n",
    "    b_conv2 = weight_variable([64])                                                                                                                                                                          \n",
    "                                                                                                                                                                                                             \n",
    "    W_conv3 = weight_variable([3, 3, 64, 128])                                                                                                                                                               \n",
    "    b_conv3 = weight_variable([128])                                                                                                                                                                         \n",
    "                                                                                                                                                                                                             \n",
    "    W_conv4 = weight_variable([3, 3, 128, 256])                                                                                                                                                              \n",
    "    b_conv4 = weight_variable([256])                                                                                                                                                                         \n",
    "                                                                                                                                                                                                             \n",
    "    W_conv5 = weight_variable([3, 3, 256, 256])                                                                                                                                                              \n",
    "    b_conv5 = weight_variable([256])                                                                                                                                                                         \n",
    "                                                                                                                                                                                                             \n",
    "    W_fcl = weight_variable([3 * 3 * 256, 1024])                                                                                                                                                             \n",
    "    b_fcl = weight_variable([1024])                                                                                                                                                                          \n",
    "    fcl = []                                                                                                                                                                                                 \n",
    "    flat_list = []                                                                                                                                                                                           \n",
    "    for train_img in [anchor_input, positive_input, negative_input]:                                                                                                                                         \n",
    "        # CNN1                                                                                                                                                                                               \n",
    "        h_conv1 = tf.nn.relu(tf.nn.conv2d(train_img, W_conv1, [1,1,1,1], 'SAME') + b_conv1)                                                                                                                  \n",
    "        h_pool1 = max_pool_2x2(h_conv1)                                                                                                                                                                      \n",
    "                                                                                                                                                                                                             \n",
    "        # CNN2                                                                                                                                                                                               \n",
    "        h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, [1,1,1,1], 'SAME') + b_conv2)                                                                                                                    \n",
    "        h_pool2 = max_pool_2x2(h_conv2)                                                                                                                                                                      \n",
    "                                                                                                                                                                                                             \n",
    "        # CNN3                                                                                                                                                                                               \n",
    "        h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, [1,1,1,1], 'SAME') + b_conv3)                                                                                                                    \n",
    "        h_pool3 = max_pool_2x2(h_conv3)                                                                                                                                                                      \n",
    "                                                                                                                                                                                                             \n",
    "        # CNN4                                                                                                                                                                                               \n",
    "        h_conv4 = tf.nn.relu(tf.nn.conv2d(h_pool3, W_conv4, [1,1,1,1], 'SAME') + b_conv4)                                                                                                                    \n",
    "        h_pool4 = max_pool_2x2(h_conv4)                                                                                                                                                                      \n",
    "                                                                                                                                                                                                             \n",
    "        # CNN5                                                                                                                                                                                               \n",
    "        h_conv5 = tf.nn.relu(tf.nn.conv2d(h_pool4, W_conv5, [1,1,1,1], 'SAME') + b_conv5)                                                                                                                    \n",
    "        h_pool5 = avg_pool_2x2(h_conv5)                                                                                                                                                                      \n",
    "                                                                                                                                                                                                             \n",
    "        # flatten the CNN                                                                                                                                                                                    \n",
    "        h_pool5_flat = tf.reshape(h_pool5, [-1, 3 * 3 * 256])                                                                                                                                                \n",
    "                                                                                                                                                                                                             \n",
    "        h_fcl = tf.nn.relu(tf.matmul(h_pool5_flat, W_fcl) + b_fcl)                                                                                                                                           \n",
    "        h_fcl = tf.nn.dropout(h_fcl, keep_prob=keepProb)                                                                                                                                                     \n",
    "                                                                                                                                                                                                             \n",
    "        fcl.append(h_fcl)                                                                                                                                                                                    \n",
    "        flat_list.append(h_pool5_flat)                                                                                                                                                                       \n",
    "                                                                                                                                                                                                             \n",
    "    anchor_image = fcl[0]                                                                                                                                                                                    \n",
    "    positive_image = fcl[1]                                                                                                                                                                                  \n",
    "    negative_image = fcl[2]                                                                                                                                                                                  \n",
    "    d_pos = tf.reduce_sum(tf.square(anchor_image - positive_image), 1)                                                                                                                                       \n",
    "    d_neg = tf.reduce_sum(tf.square(anchor_image - negative_image), 1)                                                                                                                                       \n",
    "                                                                                                                                                                                                             \n",
    "    triplet_loss_val = tf.maximum(0., margin + d_pos - d_neg)                                                                                                                                                \n",
    "    triplet_loss = tf.reduce_mean(triplet_loss_val)                                                                                                                                                          \n",
    "                                                                                                                                                                                                             \n",
    "    decay_lr = tf.train.exponential_decay(learningRate, global_step, 100, 0.975, staircase=True)                                                                                                             \n",
    "    train_step = tf.train.AdamOptimizer(decay_lr).minimize(triplet_loss, global_step=global_step)                                                                                                            \n",
    "                                                                                                                                                                                                             \n",
    "    sess.run(tf.global_variables_initializer())                                                                                                                                                              \n",
    "    print( \"start AdamOptimizer...\" )                                                                                                                                                                                    \n",
    "                                                                                                                                                                                                             \n",
    "    # Iteration begins                                                                                                                                                                                       \n",
    "    for i in range(iter_time):                                                                                                                                                                               \n",
    "        # Select train batch                                                                                                                                                                                 \n",
    "        batch_anchor = anchor_img[random.sample(pos_list, batch_size)]                                                                                                                                       \n",
    "        batch_pos = pos_img[random.sample(pos_list, batch_size)]                                                                                                                                             \n",
    "        batch_neg = neg_img[random.sample(neg_list, batch_size)]                                                                                                                                             \n",
    "                                                                                                                                                                                                             \n",
    "        # Train a step                                                                                                                                                                                       \n",
    "        train_step.run(feed_dict={anchor_input: batch_anchor, positive_input: batch_pos, negative_input: batch_neg})                                                                                         \n",
    "                                                                                                                                                                                                             \n",
    "        lossVal = triplet_loss.eval( feed_dict={anchor_input: batch_anchor, positive_input: batch_pos, negative_input: batch_neg} )                                                                          \n",
    "        print (\"lossVal: \", lossVal)                                                                                                                                                                           \n",
    "                                                                                                                                                                                                             \n",
    "        if i is not 1 and i % 5000 is 1:                                                                                                                                                                     \n",
    "            print (\"iteration at: \" + str(i))                                                                                                                                                                 \n",
    "            print (\"Start Evaluate the feature...\")                                                                                                                                                          \n",
    "                                                                                                                                                                                                             \n",
    "            # Train data features                                                                                                                                                                            \n",
    "            # positive Train Features                                                                                                                                                                        \n",
    "            featuresRes_pos_Tr = fcl[1].eval( feed_dict={anchor_input: pos_img, positive_input: pos_img, negative_input: neg_img })                                                                          \n",
    "            # negative Train Features                                                                                                                                                                        \n",
    "            featuresRes_neg_Tr = fcl[2].eval( feed_dict={anchor_input: pos_img, positive_input: pos_img, negative_input: neg_img})                                                                           \n",
    "                                                                                                                                                                                                             \n",
    "            # positive validation features                                                                                                                                                                   \n",
    "            featuresRes_pos_Va = fcl[1].eval( feed_dict={anchor_input: pos_img_Va, positive_input: pos_img_Va, negative_input: neg_img_Va} )                                                                 \n",
    "            # negative validation features                                                                                                                                                                   \n",
    "            featuresRes_neg_Va = fcl[2].eval( feed_dict={anchor_input: pos_img_Va, positive_input: pos_img_Va, negative_input: neg_img_Va})                                                                  \n",
    "                                                                                                                                                                                                             \n",
    "            print (\"start SVM...\")                                                                                                                                                                          \n",
    "            Testacc, SVMtime, Recall, y_p, train_acc = doSVM(featuresRes_pos_Tr, featuresRes_neg_Tr, featuresRes_pos_Va, featuresRes_neg_Va, SVMReg)                                                         \n",
    "            logging.info(\"At iteration: \" + str(i) + \" loss value: \" + str(lossVal) + \" recall: \" + str(Recall) + \" Train Accuracy: \" + str(train_acc) + \", Test Accuracy: \" + str(Testacc) )                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins  = [1, 10]\n",
    "batchSizes = [16,32]\n",
    "dropOutProbs = [0.01, 0.2]\n",
    "learnRates = [5e-4, 2e-4]\n",
    "SVMRegularizations = [0.1,1]\n",
    "iterationTime = 10001\n",
    "for i in range(100):\n",
    "    margin = random.choice(margins)\n",
    "    batch_size = random.choice(batchSizes)\n",
    "    dropOutProb = random.choice(dropOutProbs)\n",
    "    learnRate = random.choice(learnRates)\n",
    "    SVMreg = random.choice(SVMRegularizations)\n",
    "    logging.info( \"time: \" + str(i) + \"margin: \" + str(margin) + \" batchSize: \" + str(batch_size) + \" dropOut: \" + str(dropOutProb) + \" learnRate: \" + str(learnRate) + \" SVM REG: \"  + str(SVMreg) )\n",
    "    testErr = RunAndTest(pos_img_Tr, neg_img_Tr, anchor_img_Tr, pos_list_Tr, neg_list_Tr, pos_img_Te, neg_img_Te, pos_img_Va, neg_img_Va, margin=margin, keepProb=dropOutProb, learningRate=learnRate, iter_time= iterationTime, batch_size = batch_size, SVMReg = SVMreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with loss function result analysis\n",
    "The result from CNN network can achieve 66.8% accuracy, it demonstrates the effectiveness of the feature selection by using triplet loss to train the convolutional network, but it is still not as good as expected, through we have paid lots of efforts on tuning the parameters and optimizing the network structure. We have the summaries that the reasons might be:\n",
    "\n",
    "1. Structure of network: The convolution network is not deep and wide enough to extract best features.  With the network only have 5 convolutional layers and 1 fully connected layer, the best features are not easily extracted.\n",
    "\n",
    "2. Parameters tuning: Selecting the right parameters actually takes lots of more time and resources. We do have the implemented the stochastic validation to find the good parameters, but it is not enough. The choice set of each parameter are come from our experiments and experience, in reality we need to try much more different parameters. Also, we need to do more runs and validations to find the best parametres.\n",
    "\n",
    "3. Network Optimization: There are also lots of things to tune in the network. e.g. In convolutional layers, the size of kernel, the pooling methods, the activation function, the number of channels etc. In whole network, the number of convolutional or fully connected layers, the number of neurons in fully connected layers, etc.\n",
    "\n",
    "To achieve the best performance there are lots of work to do, unfortunately, at this time we are not able to do all of them due to time and resources limit. We implemented the simplified network to indicates the effectiveness of this approach, and we do think the capability of this approaches once it got well trained and tuned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
